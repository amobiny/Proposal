\documentclass[a4paper, 11pt]{article}

\usepackage[english]{babel}
\usepackage[margin=1in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{csquotes}

\begin{document}
\begin{titlepage}
\begin{center}
\vspace*{1cm}
\textbf{\Huge{Learning to Learn}}\\
\vspace{0.5cm}
\textbf{\Large{Using Deep Networks with Memory Capacity}}
       
\vspace{2cm}
\textbf{Dr. Hien Nguyen V}
        
\textbf{Aryan Mobiny}
        
\vspace{1cm}        
Department of Electrical and Computer Engineering\\
University of Houston\\
Houston, TX

\vspace{5cm}
\textbf{Abstract}
\end{center}
        
The goal of the proposed work is to replace the hand-designed update rule of the standard optimization algorithms (such as gradient descent, Newton\textsc{\char13}s method, etc.) with a learned update rule. Our project will investigate computational methods that can effectively learn update rules given a family of functions. Most of the current optimization algorithms have hand-designed update rules that exploit structure of a specific problem at hand. They, however, do not work well for problems outside of their scope. For example, the deep learning community and the sparsity community use very different set of optimization algorithms. We hypothesize that casting the design of optimization algorithm as a learning problem, called learning to learn or meta-learning, will allow the algorithm to learn to exploit structure of new optimization problems in an automatic way. We model the optimizer by recurrent neural networks such as Long Short Term Memory (LSTM) network and Differential Neural Computer (DNC). The optimizer is learned by varying weights of these networks. The recurrent networks enable our algorithm to compute the update rule based on not only the input data, but also the complete history of past updates. Our project proposes a novel objective function where we maximize the expected convergence rate instead of the original cost functions. We will evaluate our algorithms by training complex deep networks on large-scale database such as ImageNet and Visual7W.

        
\end{titlepage}

\section{Introduction}
Phrases like \enquote{I have experience in ...}, \enquote{This is similar to ...} or \enquote{This is a typical case of ...} imply that the person making such statements learns the task at hand faster or more accurately than an inexperienced human. This learning enhancement results from solution regularities in a problem domain. In a conventional machine learning approach, the learning algorithm mostly does not take into account previous learning experiences despite the fact that methods similar to human reasoning are expected to yield better performance. The use of previous learning experiences in inductive reasoning is known as\enquote{knowledge transfer}. Here, we focus on one of the most appealing topics in knowledge transfer research field: \enquote{\textit{meta-learning}} or \enquote{\textit{learning to learn}}.

\emph{Learning to learn} is an exciting new research direction in designing optimization algorithms that can change the way they \emph{generalize}. Given a family of tasks, an algorithm is said to learn to learn if its performance at each task improves with experience and with the number of tasks. Put differently, an ordinary learning algorithm whose performance doesn\textsc{\char13}t depend on the number of learning tasks, which hence would not benefit from the presence of other learning tasks, is not said learn to learn. For an algorithm to fit this definition, some kind of transfer must occur between multiple tasks that must have a positive impact on expected task-performance. Generally speaking, learning to learn or meta-learning refers to a scenario in which an agent learns at two levels, each associated with different time scales. Rapid learning occurs within a task, for example, when learning to accurately classify emph{within} a particular data set. This learning is guided by a slower learning where knowledge is accrued gradually \emph{across} tasks to captures the way that task structure varies across target domains.

\vspace{1.5cm}
\section{Methodology}

In machine learning, tasks can be expressed as the problem of optimizing a cost function $f(\theta)$ and the goal is to find parameters $\theta$ that minimizes the cost $(\theta^*=\text{argmin}_{\theta} \ f(\theta))$. Among all possible methods to be applied, gradient descent is the standard approach for differentiable functions, resulting in the following sequence of updates:
$$\theta_{t+1}=\theta_{t}-\alpha_t(\nabla f(\theta_{t}))$$
where $\nabla f(\theta_{t}$ denotes the gradient of the function. While there are various types of problems of interest in different research areas, much of the focus in optimization works is based around such hand-designed update rules which make it suitable only to the specific problem at hand. In other words, exploiting the structure of the problem of interest for designing the optimizer comes at the expense of potentially poor performance on problems outside of that scope. The process of designing an optimizer is time and labor intensive, and limited to a small number of knowledgable scientists.

Our project aims to use meta-learning to invent a learning algorithm that will perform well on any class of functions. To achieve that goal, our algorithm must be able automatically exploit structure of given optimizerion problems. To this end, we propose to replace the hand-designed update rule of gradient descent with optimizer $g$ which can be learned. The resulting updates of the objective function $f$ is of the following form:
$$\theta_{t+1}=\theta_{t}+g_t(\nabla f(\theta_{t}),\varphi)$$
where $\varphi$ denotes the set of parameters of optimizer g. This notation enables us to cast the design of optimization algorithm as a learning problem so that the resulting optimizers are specialized to particular classes of functions. 

In order to model the update rule $g$, we can rely on the ability of deep networks to generalize to new examples by learning interesting sub-structures. Specifically, it\textsc{\char13}s been proposed that recurrent neural networks (RNN) are quite suitable to model the update rule $g$. Having internal memory enables RNN to maintain its own states and learn dynamic update rules which integrate information from the history of gradients. 

Another aspect of high importance which is often overlooked has to do with the speed at which the algorithm converge.  In this sense, \enquote{rate of convergence}, the speed at which a convergent sequence approaches its limit, gives us a measure of the efficiency of the designed optimizer. We believe that while much of the focus in optimization works is on minimizing the cost function, inserting the rate of convergence in the problem of learning the optimizer and maximizing it is the key to achieve a suitable optimization algorithm. 
Given a sequence $x_1,\ x_2,\ \ldots,\ x_n$ converging to a target value r, the rate of convergence of the sequence ($\alpha$) is defined as:

\textbf{****change this to formula without $\lambda$****}
$$\alpha\approx\frac{(log{|x_{n+1}-r|-\lambda)}}{log{|x_n-r|}}$$ 
Where $\lambda$ is a positive real number. Rewriting the formula by inserting the objective function results in
$$\mathbb{E}[\alpha]=\mathbb{E}\left[\frac{(log|f(\theta_{n+1}(f,\varphi))-r|-\lambda)}{log|f(\theta_n (f,\varphi))-r|}\right]$$
In this setting, r can be estimated from the current literature or we can set it to zero by default. 
To evaluate the performance of the proposed optimization algorithm, it needs to be implemented to several classes of experiments to see how well it can derive the learning algorithms from scratch. We will investigate whether our learned algorithm is able to outperform generic, hand-designed competitors on the problems for which they are trained, and also generalize well to new problems with similar structure. 

Our goal is to demonstrate this on various classes of problems, from optimizing simple convex functions to functions with sparsity and group sparsity constraints. Specifically, we plan to apply our method on the following regression task: 
$$\underset{\Gamma}{\operatorname{argmax}} \| \mathbf{Y} - \mathbf{D} \mathbf{\Gamma} \|^2 + \Omega (\mathbf{\Gamma})$$
where $\mathbf{Y}$, $\mathbf{D}$, and $\mathbf{\Gamma}$ denote the input data, dictionary, and sparse coefficients, respectively. We will explore the optimization with different $\Omega (\mathbf{\Gamma})$ settings such as $\|\mathbf{\Gamma}\|_0$ and $\|\mathbf{\Gamma}\|_{1,2}$, corresponding to sparsity and group-sparsity constraints, respectively.

Recently, deep learning has become extremely active research topic due to its remarkable performance an array of important tasks, including image classification and natural language processing. However, many proposed deep networks are difficult to optimize. Heuristic techniques such as dropout and adding noise to the gradients are often used to avoid getting stuck at bad local minima. We will investigate our learned neural optimizer on training complex deep networks such as Differential Neural Computer and Neural Programmer. We will compare the performance of our optimizer with state-of-the-art methods used in deep learning community such as RMSprop, ADAM, and Nestrov\textsc{\char13}s accelerated gradient (NAG). Testing and evaluating the generalization of the learned optimizer to different architectures can be done for different applications and purposes, e.g., regression, image classification, image style transfer and synthesis, and by using various sources of data such as MNIST and ImageNet. 

\textbf{****Describe how we use the high performing computing facility****}

\end{document}
